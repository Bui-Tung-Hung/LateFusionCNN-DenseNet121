{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nth3GK5XpiTv"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.applications import DenseNet121\n",
        "from keras.models import Model, Sequential\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.metrics import AUC\n",
        "# from keras.metrics import f1_score\n",
        "from keras.metrics import Precision\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import F1Score, AUC, Precision, Recall\n",
        "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f9oA0qsIBUb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "\n",
        "def get_mel_spectrogram(file_directory, sr=22050, desired_shape=(244, 244, 3)):\n",
        "    \"\"\"\n",
        "    Get mel-spectrogram (db) with desired shape\n",
        "    Input: file_directory: str, sr: int, desired_shape: tuple\n",
        "    Output: mel_spec: numpy array\n",
        "    \"\"\"\n",
        "    src, sc = librosa.load(file_directory, sr=22050)\n",
        "    mel_gram = librosa.feature.melspectrogram(y=src, sr=22050)\n",
        "\n",
        "    # Convert to decibels\n",
        "    mel_spec_db = librosa.power_to_db(mel_gram, ref=np.max)\n",
        "\n",
        "    # Resize to desired shape\n",
        "    mel_spec_resized = resize(mel_spec_db, desired_shape)\n",
        "\n",
        "    return mel_spec_resized\n",
        "\n",
        "def process_audio_files(audio_dir):\n",
        "    ''' Process audio files in the specified directory and return log-mel spectrograms and labels.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    audio_dir: str\n",
        "        Path to the directory containing audio files.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing a list of log-mel spectrograms and corresponding labels.\n",
        "    '''\n",
        "\n",
        "    # List to store log-mel spectrograms and corresponding labels\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over the subdirectories (genres)\n",
        "    for genre_folder in os.listdir(audio_dir):\n",
        "        genre_path = os.path.join(audio_dir, genre_folder)\n",
        "        if os.path.isdir(genre_path):  # Check if it's a directory\n",
        "            # Iterate over all audio files in the genre directory\n",
        "            for filename in os.listdir(genre_path):\n",
        "                if filename.endswith('.wav'):  # Assuming all audio files are in .wav format\n",
        "                    file_path = os.path.join(genre_path, filename)\n",
        "\n",
        "                    # Compute log-mel spectrogram for the current audio file\n",
        "                    log_mel_spectrogram = get_mel_spectrogram(file_path)\n",
        "\n",
        "                    # Append log-mel spectrogram to data list\n",
        "                    data.append(log_mel_spectrogram)\n",
        "\n",
        "                    # Append label to labels list (genre folder name)\n",
        "                    labels.append(genre_folder)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# Directory containing audio files\n",
        "audio_dir = '/content/drive/MyDrive/Music_Project/music_data/VNTM3'\n",
        "\n",
        "# Process audio files and retrieve log-mel spectrograms and labels\n",
        "data, labels = process_audio_files(audio_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PucYcIpmpyiA"
      },
      "outputs": [],
      "source": [
        "# Split data into training and combined validation-evaluation set\n",
        "x_train, x_combined, y_train, y_combined = train_test_split(data, labels, test_size=0.2, random_state=42, shuffle=True, stratify =labels)\n",
        "\n",
        "# Split combined set into validation and evaluation sets\n",
        "x_validate, x_test, y_validate, y_test = train_test_split(x_combined, y_combined, test_size=0.5, random_state=42, shuffle=True, stratify =y_combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt8EWnLyp0ig"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_binarizer = LabelBinarizer()\n",
        "# Fit and transform training labels\n",
        "y_train_one_hot = label_binarizer.fit_transform(y_train)\n",
        "y_validate_one_hot = label_binarizer.transform(y_validate)\n",
        "y_test_one_hot = label_binarizer.transform(y_test)\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "# Fit and transform evaluation labels\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "y_test_one_hot = label_binarizer.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo2CfyioRcxA"
      },
      "outputs": [],
      "source": [
        "X_test = np.array(x_test, dtype=np.float32)\n",
        "X_train = np.array(x_train, dtype = np.float32)\n",
        "X_validate = np.array(x_validate, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn5J1ZdWqBNg",
        "outputId": "43a738bb-82ee-4315-c6d8-6de0a51b683b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29084464/29084464 [==============================] - 1s 0us/step\n",
            "Epoch 1/100\n",
            "63/63 [==============================] - 20s 147ms/step - loss: 1.1805 - accuracy: 0.6218 - val_loss: 0.5279 - val_accuracy: 0.8320\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 3s 51ms/step - loss: 0.6179 - accuracy: 0.7774 - val_loss: 0.4778 - val_accuracy: 0.7960\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 3s 51ms/step - loss: 0.4912 - accuracy: 0.8229 - val_loss: 0.3345 - val_accuracy: 0.8920\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 3s 51ms/step - loss: 0.4269 - accuracy: 0.8449 - val_loss: 0.2771 - val_accuracy: 0.9120\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 3s 48ms/step - loss: 0.3988 - accuracy: 0.8549 - val_loss: 0.4370 - val_accuracy: 0.8400\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 3s 49ms/step - loss: 0.3715 - accuracy: 0.8679 - val_loss: 0.3225 - val_accuracy: 0.8880\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 3s 52ms/step - loss: 0.3384 - accuracy: 0.8839 - val_loss: 0.2448 - val_accuracy: 0.9080\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 3s 51ms/step - loss: 0.2994 - accuracy: 0.8909 - val_loss: 0.1985 - val_accuracy: 0.9480\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 3s 49ms/step - loss: 0.3035 - accuracy: 0.8944 - val_loss: 0.2090 - val_accuracy: 0.9200\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 3s 52ms/step - loss: 0.2884 - accuracy: 0.8924 - val_loss: 0.1821 - val_accuracy: 0.9280\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 3s 49ms/step - loss: 0.2762 - accuracy: 0.8994 - val_loss: 0.1864 - val_accuracy: 0.9360\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 3s 51ms/step - loss: 0.2629 - accuracy: 0.9065 - val_loss: 0.1625 - val_accuracy: 0.9400\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 3s 48ms/step - loss: 0.2334 - accuracy: 0.9155 - val_loss: 0.1915 - val_accuracy: 0.9320\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 3s 49ms/step - loss: 0.2511 - accuracy: 0.9110 - val_loss: 0.1756 - val_accuracy: 0.9320\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 3s 49ms/step - loss: 0.2115 - accuracy: 0.9175 - val_loss: 0.1625 - val_accuracy: 0.9280\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 3s 48ms/step - loss: 0.1939 - accuracy: 0.9295 - val_loss: 0.2161 - val_accuracy: 0.9080\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 3s 51ms/step - loss: 0.2009 - accuracy: 0.9280 - val_loss: 0.1762 - val_accuracy: 0.9280\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f663e7bc550>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model = DenseNet121(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(244,244,3))\n",
        "\n",
        "early = EarlyStopping(monitor='val_loss',\n",
        "                      min_delta=0.0001,\n",
        "                      mode='min',\n",
        "                      verbose=0,\n",
        "                      patience=5,\n",
        "                      baseline= None,\n",
        "                      restore_best_weights= True)\n",
        "\n",
        "top_model = Sequential([\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model = Model(inputs=base_model.input,\n",
        "              outputs=top_model(base_model.output))\n",
        "\n",
        "# Freeze base layers to prevent retraining\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "#model.summary()\n",
        "\n",
        "model.fit(X_train, y_train_one_hot,\n",
        "          epochs=100,\n",
        "          validation_data=(X_validate, y_validate_one_hot),\n",
        "          callbacks=[early],\n",
        "          shuffle=True,\n",
        "          verbose=1,\n",
        "          batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At6OBO3lqGdu",
        "outputId": "43f7209d-9fbc-48d4-a302-d7ca189b45bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 2s 38ms/step\n",
            "AUC ROC: 0.995938\n",
            "F1 Score: 0.95263547\n",
            "Precision Score 0.9508197\n",
            "Accuracy Score 0.952\n",
            "Recall Score: 0.952\n"
          ]
        }
      ],
      "source": [
        "predict = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Convert one-hot encoded predicted labels back to original labels\n",
        "test_predictions_labels = label_encoder.inverse_transform(np.argmax(predict, axis=1))\n",
        "# Convert one-hot encoded predicted labels back to original labels\n",
        "test_predictions_labels_encoded = label_encoder.inverse_transform(np.argmax(predict, axis=1))\n",
        "\n",
        "\n",
        "# Calculate AUC_ROC\n",
        "auc_roc = AUC()\n",
        "auc_roc.update_state(y_test_one_hot, predict)\n",
        "print(\"AUC ROC:\", auc_roc.result().numpy())\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1 = F1Score()\n",
        "f1.update_state(y_test_one_hot, predict)\n",
        "print(\"F1 Score:\", np.mean(f1.result().numpy()))\n",
        "\n",
        "# Calculate Precision\n",
        "pre = Precision()\n",
        "pre.update_state(y_test_one_hot, predict)\n",
        "print(\"Precision Score\", pre.result().numpy())\n",
        "\n",
        "# Accuracy Score\n",
        "acc = accuracy_score(y_test, test_predictions_labels)\n",
        "print(\"Accuracy Score\", acc)\n",
        "\n",
        "print(\"Recall Score:\", recall_score(y_test_encoded, np.argmax(predict, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNUopqdoyuHV"
      },
      "outputs": [],
      "source": [
        "test_predictions_labels_encoded_list = test_predictions_labels_encoded.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c909kbK5qSkO",
        "outputId": "e46e91c1-df5c-4165-f984-36ee51419908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample\t\tTrue Label\tPredicted Label\n",
            "----------------------------------------\n",
            "1\t\tcheo\t\tcheo\n",
            "2\t\tcailuong\t\tcailuong\n",
            "3\t\tcheo\t\tcheo\n",
            "4\t\tchauvan\t\tchauvan\n",
            "5\t\tcatru\t\tcatru\n",
            "6\t\thatxam\t\thatxam\n",
            "7\t\tcheo\t\tcheo\n",
            "8\t\tcailuong\t\tcailuong\n",
            "9\t\thatxam\t\thatxam\n",
            "10\t\tcailuong\t\tcailuong\n",
            "11\t\tcailuong\t\tcailuong\n",
            "12\t\thatxam\t\thatxam\n",
            "13\t\tchauvan\t\tchauvan\n",
            "14\t\tcheo\t\tcheo\n",
            "15\t\tcheo\t\tcheo\n",
            "16\t\tcatru\t\tcatru\n",
            "17\t\tcheo\t\tcheo\n",
            "18\t\thatxam\t\thatxam\n",
            "19\t\thatxam\t\tchauvan\n",
            "20\t\tchauvan\t\tchauvan\n",
            "21\t\tcatru\t\tcatru\n",
            "22\t\tchauvan\t\tchauvan\n",
            "23\t\thatxam\t\thatxam\n",
            "24\t\thatxam\t\thatxam\n",
            "25\t\thatxam\t\thatxam\n",
            "26\t\tchauvan\t\tchauvan\n",
            "27\t\tcatru\t\tcatru\n",
            "28\t\tchauvan\t\tchauvan\n",
            "29\t\tcheo\t\thatxam\n",
            "30\t\tcheo\t\tcheo\n",
            "31\t\thatxam\t\thatxam\n",
            "32\t\tchauvan\t\tchauvan\n",
            "33\t\thatxam\t\thatxam\n",
            "34\t\thatxam\t\tchauvan\n",
            "35\t\tcailuong\t\tcailuong\n",
            "36\t\tcailuong\t\tcailuong\n",
            "37\t\tcatru\t\tcatru\n",
            "38\t\tchauvan\t\thatxam\n",
            "39\t\tcheo\t\tcheo\n",
            "40\t\tchauvan\t\tchauvan\n",
            "41\t\tcatru\t\tcatru\n",
            "42\t\thatxam\t\thatxam\n",
            "43\t\tcheo\t\tcheo\n",
            "44\t\tcheo\t\tcheo\n",
            "45\t\tchauvan\t\tchauvan\n",
            "46\t\thatxam\t\thatxam\n",
            "47\t\thatxam\t\thatxam\n",
            "48\t\thatxam\t\thatxam\n",
            "49\t\tcheo\t\tcheo\n",
            "50\t\tcailuong\t\tcailuong\n",
            "51\t\tchauvan\t\tchauvan\n",
            "52\t\tcailuong\t\tcailuong\n",
            "53\t\tcailuong\t\tcailuong\n",
            "54\t\tchauvan\t\tchauvan\n",
            "55\t\tcatru\t\tcatru\n",
            "56\t\thatxam\t\thatxam\n",
            "57\t\tchauvan\t\tchauvan\n",
            "58\t\tcailuong\t\tcailuong\n",
            "59\t\tcatru\t\tcatru\n",
            "60\t\tcailuong\t\tcailuong\n",
            "61\t\tcheo\t\tcheo\n",
            "62\t\tcheo\t\tcheo\n",
            "63\t\tcatru\t\tcatru\n",
            "64\t\tcailuong\t\tcailuong\n",
            "65\t\tcatru\t\thatxam\n",
            "66\t\tcailuong\t\thatxam\n",
            "67\t\tchauvan\t\tchauvan\n",
            "68\t\tcatru\t\tcatru\n",
            "69\t\tcailuong\t\tcailuong\n",
            "70\t\tcatru\t\tcatru\n",
            "71\t\tcatru\t\tcatru\n",
            "72\t\tcheo\t\tcheo\n",
            "73\t\tcheo\t\tcheo\n",
            "74\t\thatxam\t\thatxam\n",
            "75\t\tcailuong\t\thatxam\n",
            "76\t\thatxam\t\thatxam\n",
            "77\t\thatxam\t\thatxam\n",
            "78\t\tchauvan\t\tchauvan\n",
            "79\t\thatxam\t\thatxam\n",
            "80\t\tchauvan\t\tchauvan\n",
            "81\t\tchauvan\t\tchauvan\n",
            "82\t\thatxam\t\thatxam\n",
            "83\t\thatxam\t\thatxam\n",
            "84\t\tcailuong\t\thatxam\n",
            "85\t\tcailuong\t\tcailuong\n",
            "86\t\tcatru\t\tcatru\n",
            "87\t\tcailuong\t\tcailuong\n",
            "88\t\tchauvan\t\tchauvan\n",
            "89\t\tcheo\t\tcheo\n",
            "90\t\tchauvan\t\tchauvan\n",
            "91\t\tchauvan\t\tcailuong\n",
            "92\t\tcatru\t\tcatru\n",
            "93\t\tcailuong\t\tcailuong\n",
            "94\t\tcatru\t\tcheo\n",
            "95\t\thatxam\t\thatxam\n",
            "96\t\tcheo\t\tcheo\n",
            "97\t\tcailuong\t\tcailuong\n",
            "98\t\thatxam\t\thatxam\n",
            "99\t\tcheo\t\tcheo\n",
            "100\t\tcailuong\t\tcailuong\n",
            "101\t\tcheo\t\tcheo\n",
            "102\t\tcatru\t\tcatru\n",
            "103\t\thatxam\t\thatxam\n",
            "104\t\tcheo\t\tcheo\n",
            "105\t\thatxam\t\thatxam\n",
            "106\t\thatxam\t\thatxam\n",
            "107\t\tcheo\t\tcheo\n",
            "108\t\tcheo\t\tcheo\n",
            "109\t\tchauvan\t\tchauvan\n",
            "110\t\tcatru\t\tcatru\n",
            "111\t\tcatru\t\thatxam\n",
            "112\t\tcailuong\t\tcailuong\n",
            "113\t\tchauvan\t\tchauvan\n",
            "114\t\thatxam\t\thatxam\n",
            "115\t\tcailuong\t\tcailuong\n",
            "116\t\tchauvan\t\tchauvan\n",
            "117\t\tchauvan\t\tchauvan\n",
            "118\t\tcheo\t\tcheo\n",
            "119\t\tchauvan\t\tchauvan\n",
            "120\t\tcailuong\t\tcailuong\n",
            "121\t\tcheo\t\tcheo\n",
            "122\t\thatxam\t\thatxam\n",
            "123\t\tcailuong\t\tcailuong\n",
            "124\t\tcheo\t\tcheo\n",
            "125\t\tchauvan\t\tchauvan\n",
            "126\t\tcatru\t\tcatru\n",
            "127\t\tcheo\t\tcheo\n",
            "128\t\thatxam\t\thatxam\n",
            "129\t\tchauvan\t\tchauvan\n",
            "130\t\tchauvan\t\tchauvan\n",
            "131\t\tcheo\t\tcheo\n",
            "132\t\tcailuong\t\tcailuong\n",
            "133\t\tcailuong\t\tcailuong\n",
            "134\t\tcheo\t\tcheo\n",
            "135\t\tchauvan\t\tchauvan\n",
            "136\t\tcheo\t\tcheo\n",
            "137\t\tcatru\t\tcatru\n",
            "138\t\tchauvan\t\tchauvan\n",
            "139\t\tcheo\t\tcheo\n",
            "140\t\tcheo\t\tcheo\n",
            "141\t\tcailuong\t\tcailuong\n",
            "142\t\tcheo\t\tcheo\n",
            "143\t\thatxam\t\thatxam\n",
            "144\t\tcheo\t\tcheo\n",
            "145\t\tcailuong\t\tcailuong\n",
            "146\t\tcatru\t\tcatru\n",
            "147\t\tchauvan\t\tchauvan\n",
            "148\t\tcatru\t\tcatru\n",
            "149\t\tcatru\t\tcatru\n",
            "150\t\tchauvan\t\tchauvan\n",
            "151\t\thatxam\t\thatxam\n",
            "152\t\tcailuong\t\tcailuong\n",
            "153\t\tcatru\t\tcatru\n",
            "154\t\tcailuong\t\tcailuong\n",
            "155\t\tcatru\t\tcatru\n",
            "156\t\tcheo\t\tcheo\n",
            "157\t\tcailuong\t\tcailuong\n",
            "158\t\tcatru\t\tcatru\n",
            "159\t\thatxam\t\thatxam\n",
            "160\t\thatxam\t\thatxam\n",
            "161\t\tcailuong\t\tcailuong\n",
            "162\t\tcatru\t\tcatru\n",
            "163\t\tchauvan\t\tchauvan\n",
            "164\t\tcailuong\t\tcailuong\n",
            "165\t\tcheo\t\tcheo\n",
            "166\t\tcatru\t\tcatru\n",
            "167\t\tcailuong\t\tcailuong\n",
            "168\t\tcailuong\t\tcailuong\n",
            "169\t\tchauvan\t\tchauvan\n",
            "170\t\thatxam\t\thatxam\n",
            "171\t\tchauvan\t\tchauvan\n",
            "172\t\tcatru\t\tcatru\n",
            "173\t\tcheo\t\tcheo\n",
            "174\t\tcatru\t\tcatru\n",
            "175\t\tcheo\t\tcheo\n",
            "176\t\tcatru\t\tcatru\n",
            "177\t\tcailuong\t\tcailuong\n",
            "178\t\tcheo\t\tcheo\n",
            "179\t\tchauvan\t\tchauvan\n",
            "180\t\thatxam\t\thatxam\n",
            "181\t\tcailuong\t\tcailuong\n",
            "182\t\tchauvan\t\tchauvan\n",
            "183\t\thatxam\t\thatxam\n",
            "184\t\tcatru\t\tcatru\n",
            "185\t\tcatru\t\tcatru\n",
            "186\t\tcheo\t\tcheo\n",
            "187\t\thatxam\t\thatxam\n",
            "188\t\tchauvan\t\tchauvan\n",
            "189\t\tchauvan\t\tchauvan\n",
            "190\t\tcailuong\t\tcailuong\n",
            "191\t\tchauvan\t\tchauvan\n",
            "192\t\tcatru\t\tcatru\n",
            "193\t\tcailuong\t\tcailuong\n",
            "194\t\tcailuong\t\tcailuong\n",
            "195\t\tcatru\t\tcatru\n",
            "196\t\tcatru\t\tcatru\n",
            "197\t\tcailuong\t\tcailuong\n",
            "198\t\tcatru\t\tcatru\n",
            "199\t\thatxam\t\thatxam\n",
            "200\t\tcheo\t\tcheo\n",
            "201\t\tchauvan\t\tchauvan\n",
            "202\t\tchauvan\t\tchauvan\n",
            "203\t\tcailuong\t\tcailuong\n",
            "204\t\thatxam\t\thatxam\n",
            "205\t\tcatru\t\tcatru\n",
            "206\t\tcatru\t\tcatru\n",
            "207\t\tchauvan\t\tchauvan\n",
            "208\t\thatxam\t\thatxam\n",
            "209\t\thatxam\t\thatxam\n",
            "210\t\thatxam\t\thatxam\n",
            "211\t\tcatru\t\tcatru\n",
            "212\t\tcheo\t\tcheo\n",
            "213\t\thatxam\t\thatxam\n",
            "214\t\tcatru\t\tcatru\n",
            "215\t\tcatru\t\tcatru\n",
            "216\t\tcailuong\t\tcailuong\n",
            "217\t\tcatru\t\tcatru\n",
            "218\t\tcatru\t\tcatru\n",
            "219\t\thatxam\t\thatxam\n",
            "220\t\tcailuong\t\tcailuong\n",
            "221\t\tcheo\t\tcheo\n",
            "222\t\tcheo\t\tcheo\n",
            "223\t\tchauvan\t\tchauvan\n",
            "224\t\tcheo\t\tcheo\n",
            "225\t\tcatru\t\tcatru\n",
            "226\t\tcatru\t\tcatru\n",
            "227\t\tchauvan\t\tchauvan\n",
            "228\t\tcatru\t\thatxam\n",
            "229\t\thatxam\t\thatxam\n",
            "230\t\thatxam\t\thatxam\n",
            "231\t\tcheo\t\tcheo\n",
            "232\t\tcailuong\t\tcailuong\n",
            "233\t\tchauvan\t\tchauvan\n",
            "234\t\tchauvan\t\tchauvan\n",
            "235\t\tcatru\t\tcatru\n",
            "236\t\thatxam\t\thatxam\n",
            "237\t\thatxam\t\thatxam\n",
            "238\t\tchauvan\t\tchauvan\n",
            "239\t\thatxam\t\thatxam\n",
            "240\t\tcatru\t\tcatru\n",
            "241\t\tcheo\t\tcheo\n",
            "242\t\tcailuong\t\tcailuong\n",
            "243\t\tcailuong\t\tcailuong\n",
            "244\t\tchauvan\t\tchauvan\n",
            "245\t\tcailuong\t\tcailuong\n",
            "246\t\tchauvan\t\tchauvan\n",
            "247\t\tcheo\t\tcheo\n",
            "248\t\tcheo\t\tcheo\n",
            "249\t\tcheo\t\tcheo\n",
            "250\t\tcailuong\t\tcailuong\n"
          ]
        }
      ],
      "source": [
        "# Print results\n",
        "print(\"Sample\\t\\tTrue Label\\tPredicted Label\")\n",
        "print(\"-\" * 40)\n",
        "for i in range(len(y_test)):\n",
        "    print(f\"{i+1}\\t\\t{y_test[i]}\\t\\t{test_predictions_labels_encoded_list[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09IbVoOjqUOm"
      },
      "outputs": [],
      "source": [
        "# # Define the checkpoint filepath\n",
        "# checkpoint_dir = '/content/drive/MyDrive/Music_Project/music_data/checkpoint/DenseNet121'\n",
        "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# # Create a ModelCheckpoint callback\n",
        "# checkpoint_filepath = os.path.join(checkpoint_dir, 'Densenet')\n",
        "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     filepath=checkpoint_filepath,\n",
        "#     save_weights_only=True,  # Save only the model weights\n",
        "#     monitor='val_loss',  # Monitor validation loss\n",
        "#     mode='min',  # Save the model when validation loss is minimized\n",
        "#     save_best_only=True  # Save only the best model\n",
        "# )\n",
        "\n",
        "# # Assuming you have already trained and evaluated your model\n",
        "# # model.fit(train_data, epochs=10, validation_data=val_data, callbacks=[checkpoint_callback])\n",
        "\n",
        "# # After training, save the entire model\n",
        "# model.save('/content/drive/MyDrive/Music_Project/music_data/checkpoint/DenseNet121/densenet121_model_10s.h5')\n",
        "\n",
        "# # Also, save only the weights of the model\n",
        "# model.save_weights('/content/drive/MyDrive/Music_Project/music_data/checkpoint/DenseNet121/densenet121_weights_10s.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56uRJidyqWM2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Tải checkpoint đã lưu\n",
        "# checkpoint_path = \"/content/drive/MyDrive/Music_Project/music_data/checkpoint/DenseNet121/densenet121_weights.h5\"\n",
        "# model_path = r\"/content/drive/MyDrive/Music_Project/music_data/checkpoint/DenseNet121/densenet121_model.h5\"\n",
        "# model = tf.keras.models.load_model(model_path)\n",
        "# model.load_weights(checkpoint_path)\n",
        "\n",
        "# # Đánh giá mô hình sau khi tải lại checkpoint\n",
        "\n",
        "# predict = model.predict(x_test)\n",
        "\n",
        "\n",
        "# # Convert one-hot encoded predicted labels back to original labels\n",
        "# test_predictions_labels = label_encoder.inverse_transform(np.argmax(predict, axis=1))\n",
        "# # Convert one-hot encoded predicted labels back to original labels\n",
        "# test_predictions_labels_encoded = label_encoder.inverse_transform(np.argmax(predict, axis=1))\n",
        "\n",
        "\n",
        "# # Calculate AUC_ROC\n",
        "# auc_roc = AUC()\n",
        "# auc_roc.update_state(y_test_one_hot, predict)\n",
        "# print(\"AUC ROC:\", auc_roc.result().numpy())\n",
        "\n",
        "# # Calculate F1 Score\n",
        "# f1 = F1Score()\n",
        "# f1.update_state(y_test_one_hot, predict)\n",
        "# print(\"F1 Score:\", np.mean(f1.result().numpy()))\n",
        "\n",
        "# # Calculate Precision\n",
        "# pre = Precision()\n",
        "# pre.update_state(y_test_one_hot, predict)\n",
        "# print(\"Precision Score\", pre.result().numpy())\n",
        "\n",
        "# # Accuracy Score\n",
        "# acc = accuracy_score(y_test, test_predictions_labels)\n",
        "# print(\"Accuracy Score\", acc)\n",
        "\n",
        "# print(\"Recall Score:\", recall_score(y_test_encoded, np.argmax(predict, axis=1), average='macro'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
